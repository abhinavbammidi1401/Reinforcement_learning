{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of the Code\n",
    "\n",
    "1. **Environment Initialization**\n",
    "The gird size and terminal stated are configurable:\n",
    "\n",
    "```bash\n",
    "env = ModelBasedGridWorld(grid_size=5, terminal_states=[(4,4)])\n",
    "```\n",
    "\n",
    "- `grid_size=5`: Create a 5x5 grid.\n",
    "- `terminal_states=[(4, 4)]`:  Sets the bottom-right corner as the goal (terminal state).\n",
    "- The agent starts at the top-left corner `(0, 0)`.\n",
    "\n",
    "2. **Actions**\n",
    "The agent has 4 discrete actions:\n",
    "\n",
    "- **Up (0)**: Move up by one cell.\n",
    "- **Right (1)**: Move right by one cell.\n",
    "- **Down (2)**: Move down by one cell.\n",
    "- **Left (3)**: Move left by one cell.\n",
    "\n",
    "Each action transitions the agent to a new state, unless:\n",
    "- It would move the agent out of bounds (e.g., moving up from the top row does nothing).\n",
    "\n",
    "3. **State Transition**\n",
    "The function step(action) defines how the agent moves within the grid:\n",
    "\n",
    "```bash\n",
    "# Movement definitions: {0: Up, 1: Right, 2: Down, 3: Left}\n",
    "moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "```\n",
    "\n",
    "- Each action modifies the agent's row and column coordinates.\n",
    "- Boundary conditions ensure the agent doesn’t move outside the grid:\n",
    "\n",
    "```bash\n",
    "next_row = max(0, min(self.grid_size - 1, next_row))\n",
    "next_col = max(0, min(self.grid_size - 1, next_col))\n",
    "```\n",
    "\n",
    "4. **Rewards**\n",
    "Rewards guide the agent’s behavior:\n",
    "- Reaching a terminal state (goal) yields a positive reward (e.g., +10).\n",
    "- Each step taken without reaching the goal incurs a negative reward (e.g., -1).\n",
    "This reward structure incentivizes the agent to reach the terminal state in as few steps as possible.\n",
    "\n",
    "5. **Terminal States**\n",
    "The episode ends when the agent reaches one of the **terminal states**:\n",
    "\n",
    "```bash\n",
    "done = next_state in self.terminal_states\n",
    "```\n",
    "\n",
    "6. **Rendering**\n",
    "The `render()` method visualizes the current state of the grid:\n",
    "\n",
    "The grid displays:\n",
    "- `A`: The agent’s current position.\n",
    "- `T`: Terminal states (goals).\n",
    "- `.`: Empty cells.\n",
    "\n",
    "Example visualization:\n",
    "```bash\n",
    "A . . . .\n",
    ". . . . .\n",
    ". . . . .\n",
    ". . . . .\n",
    ". . . . T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelBasedGridWorld:\n",
    "    def __init__(self, grid_size=5, terminal_states=None, random_seed=None, stochastic=False):\n",
    "        \"\"\"\n",
    "        Custom GridWorld environment for model-based reinforcement learning.\n",
    "        :param grid_size: Size of the grid (grid_size x grid_size).\n",
    "        :param terminal_states: List of terminal state positions (row, col).\n",
    "        :param random_seed: Random seed for reproducibility.\n",
    "        :param stochastic: Whether to use stochastic transitions.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.terminal_states = terminal_states or [(grid_size - 1, grid_size - 1)]\n",
    "        self.state = (0, 0)  # Starting state\n",
    "        self.transition_model = {}  # Transition probabilities P(s'|s, a)\n",
    "        self.reward_model = {}  # Reward function R(s, a)\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "\n",
    "        self._build_models()\n",
    "\n",
    "    def _build_models(self):\n",
    "        \"\"\"Construct the transition and reward models.\"\"\"\n",
    "        for row in range(self.grid_size):\n",
    "            for col in range(self.grid_size):\n",
    "                for action in range(4):  # Actions: 0=Up, 1=Right, 2=Down, 3=Left\n",
    "                    state = (row, col)\n",
    "                    self.transition_model[(state, action)] = self._compute_transition(state, action)\n",
    "                    self.reward_model[(state, action)] = -1  # Default step penalty\n",
    "                    \n",
    "                    if state in self.terminal_states:\n",
    "                        self.reward_model[(state, action)] = 10  # Reward for terminal state\n",
    "\n",
    "    def _compute_transition(self, state, action):\n",
    "        \"\"\"\n",
    "        Compute transition dynamics for a given state and action.\n",
    "        If stochastic=True, return a set of possible outcomes with probabilities.\n",
    "        \"\"\"\n",
    "        row, col = state\n",
    "        moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}  # Up, Right, Down, Left\n",
    "\n",
    "        if self.stochastic:\n",
    "            # Define stochastic probabilities for actions\n",
    "            probabilities = [0.8, 0.1, 0.05, 0.05]  # Action, +slight noise\n",
    "            outcomes = []\n",
    "\n",
    "            for i, (dr, dc) in moves.items():\n",
    "                next_row, next_col = row + dr, col + dc\n",
    "                next_row = max(0, min(self.grid_size - 1, next_row))\n",
    "                next_col = max(0, min(self.grid_size - 1, next_col))\n",
    "                outcomes.append(((next_row, next_col), probabilities[i]))\n",
    "            return outcomes\n",
    "        else:\n",
    "            # Deterministic movement\n",
    "            dr, dc = moves[action]\n",
    "            next_row, next_col = row + dr, col + dc\n",
    "            next_row = max(0, min(self.grid_size - 1, next_row))\n",
    "            next_col = max(0, min(self.grid_size - 1, next_col))\n",
    "            return (next_row, next_col)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action and return the next state, reward, done, and info.\n",
    "        \"\"\"\n",
    "        if self.stochastic:\n",
    "            outcomes = self.transition_model[(self.state, action)]\n",
    "            next_state = outcomes[np.random.choice(len(outcomes), p=[p for _, p in outcomes])][0]\n",
    "        else:\n",
    "            next_state = self.transition_model[(self.state, action)]\n",
    "\n",
    "        reward = self.reward_model[(self.state, action)]\n",
    "        self.state = next_state\n",
    "        done = self.state in self.terminal_states\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the environment.\"\"\"\n",
    "        grid = np.full((self.grid_size, self.grid_size), '.', dtype=str)\n",
    "        for r, c in self.terminal_states:\n",
    "            grid[r, c] = 'T'\n",
    "        row, col = self.state\n",
    "        grid[row, col] = 'A'\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "    def get_transition_model(self):\n",
    "        \"\"\"Return the transition dynamics for model-based RL.\"\"\"\n",
    "        return self.transition_model\n",
    "\n",
    "    def get_reward_model(self):\n",
    "        \"\"\"Return the reward function for model-based RL.\"\"\"\n",
    "        return self.reward_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Step: 1, Action: 2, Next State: (1, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Step: 2, Action: 2, Next State: (2, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Step: 3, Action: 2, Next State: (3, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . T\n",
      "\n",
      "Step: 4, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 5, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 6, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 7, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 8, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 9, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 10, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 11, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 12, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 13, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 14, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 15, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 16, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 17, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 18, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 19, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 20, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 21, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 22, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 23, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 24, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 25, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 26, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 27, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 28, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 29, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 30, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 31, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 32, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 33, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 34, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 35, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 36, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 37, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 38, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 39, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 40, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 41, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 42, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 43, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 44, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 45, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 46, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 47, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 48, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 49, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 50, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 51, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 52, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 53, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 54, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 55, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 56, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 57, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 58, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 59, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 60, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 61, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 62, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 63, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 64, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 65, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 66, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 67, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 68, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 69, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 70, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 71, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 72, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 73, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 74, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 75, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 76, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 77, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 78, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 79, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 80, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 81, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 82, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 83, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 84, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 85, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 86, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 87, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 88, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 89, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 90, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 91, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 92, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 93, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 94, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 95, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 96, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 97, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 98, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 99, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Step: 100, Action: 2, Next State: (4, 0), Reward: -1, Done: False\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . T\n",
      "\n",
      "Episode terminated due to maximum step limit.\n",
      "Total Reward: -100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the environment\n",
    "    env = ModelBasedGridWorld(grid_size=5, terminal_states=[(4, 4)], random_seed=42, stochastic=False)\n",
    "\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    # Simulate an episode with a maximum step count\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    max_steps = 100  # Limit the number of steps per episode\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        # Implement a simple policy: Move toward the terminal state\n",
    "        row, col = state\n",
    "        terminal_row, terminal_col = env.terminal_states[0]\n",
    "\n",
    "        if row < terminal_row:\n",
    "            action = 2  # Move Down\n",
    "        elif row > terminal_row:\n",
    "            action = 0  # Move Up\n",
    "        elif col < terminal_col:\n",
    "            action = 1  # Move Right\n",
    "        elif col > terminal_col:\n",
    "            action = 3  # Move Left\n",
    "\n",
    "        # Take the action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        print(f\"Step: {steps}, Action: {action}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "        env.render()\n",
    "\n",
    "    if steps == max_steps:\n",
    "        print(\"Episode terminated due to maximum step limit.\")\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
