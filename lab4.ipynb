{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Policy Iteration Simulation ===\n",
      "Initial Grid:\n",
      "    â–ˆ  \n",
      "    G  \n",
      "S     â–ˆ\n",
      "  â–ˆ â–ˆ  \n",
      "\n",
      "Computing optimal policy using Policy Iteration...\n",
      "\n",
      "Simulating episode using optimal policy...\n",
      "\n",
      "Final Path:\n",
      "    â–ˆ  \n",
      "â€¢ â€¢ G  \n",
      "S     â–ˆ\n",
      "â€¢ â–ˆ â–ˆ  \n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "Path length: 6 steps\n",
      "Start state: (2, 0)\n",
      "Goal state: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self, size: int = 5, obstacles: List[Tuple[int, int]] = None):\n",
    "        self.size = size\n",
    "        self.states = [(i, j) for i in range(size) for j in range(size)]\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        # Randomly place start and goal states\n",
    "        self.start_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        self.goal_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        \n",
    "        # Make sure start and goal states are different\n",
    "        while self.goal_state == self.start_state:\n",
    "            self.goal_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "            \n",
    "        # Set obstacles\n",
    "        self.obstacles = obstacles if obstacles else []\n",
    "        while len(self.obstacles) < size:  # Add random obstacles\n",
    "            obs = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "            if obs not in self.obstacles and obs != self.start_state and obs != self.goal_state:\n",
    "                self.obstacles.append(obs)\n",
    "\n",
    "    def get_next_state(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "        \"\"\"Determine next state given current state and action.\"\"\"\n",
    "        x, y = state\n",
    "        if action == 'up':\n",
    "            new_state = (max(0, x-1), y)\n",
    "        elif action == 'down':\n",
    "            new_state = (min(self.size-1, x+1), y)\n",
    "        elif action == 'left':\n",
    "            new_state = (x, max(0, y-1))\n",
    "        else:  # right\n",
    "            new_state = (x, min(self.size-1, y+1))\n",
    "            \n",
    "        # Check if new state is an obstacle\n",
    "        if new_state in self.obstacles:\n",
    "            return state\n",
    "        return new_state\n",
    "\n",
    "    def get_reward(self, state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Get reward for being in a state.\"\"\"\n",
    "        if state == self.goal_state:\n",
    "            return 100\n",
    "        elif state in self.obstacles:\n",
    "            return -50\n",
    "        return -1  # Small negative reward for each step\n",
    "\n",
    "def value_iteration(mdp: GridWorldMDP, gamma: float = 0.9, epsilon: float = 1e-6) -> Dict[Tuple[int, int], float]:\n",
    "    \"\"\"Perform value iteration to find optimal values for each state.\"\"\"\n",
    "    V = {state: 0 for state in mdp.states}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if state == mdp.goal_state:\n",
    "                continue\n",
    "                \n",
    "            # Find maximum value over all actions\n",
    "            max_value = float('-inf')\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                value = mdp.get_reward(next_state) + gamma * V[next_state]\n",
    "                max_value = max(max_value, value)\n",
    "            \n",
    "            V_new[state] = max_value\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "def get_optimal_policy(mdp: GridWorldMDP, V: Dict[Tuple[int, int], float], gamma: float = 0.9) -> Dict[Tuple[int, int], str]:\n",
    "    \"\"\"Extract optimal policy from value function.\"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in mdp.states:\n",
    "        if state == mdp.goal_state:\n",
    "            policy[state] = None\n",
    "            continue\n",
    "            \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in mdp.actions:\n",
    "            next_state = mdp.get_next_state(state, action)\n",
    "            value = mdp.get_reward(next_state) + gamma * V[next_state]\n",
    "            \n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "                \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# New Policy Iteration Function\n",
    "def policy_iteration(mdp: GridWorldMDP, gamma: float = 0.9, epsilon: float = 1e-6) -> Tuple[Dict[Tuple[int, int], str], Dict[Tuple[int, int], float]]:\n",
    "    \"\"\"\n",
    "    Perform policy iteration to find the optimal policy.\n",
    "    \n",
    "    Returns:\n",
    "    - policy: Optimal policy as a dictionary mapping each state to an action\n",
    "    - V: Optimal value function for the policy\n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = {state: random.choice(mdp.actions) for state in mdp.states if state != mdp.goal_state}\n",
    "    V = {state: 0 for state in mdp.states}\n",
    "    \n",
    "    while True:\n",
    "        # Policy Evaluation: Use the current policy to compute V\n",
    "        V = value_iteration(mdp, gamma, epsilon)  # Policy evaluation using value iteration\n",
    "        \n",
    "        # Policy Improvement: Update the policy\n",
    "        stable = True\n",
    "        for state in mdp.states:\n",
    "            if state == mdp.goal_state:\n",
    "                continue\n",
    "                \n",
    "            # Find the best action according to the current V\n",
    "            best_action = max(mdp.actions, key=lambda a: mdp.get_reward(mdp.get_next_state(state, a)) + gamma * V[mdp.get_next_state(state, a)])\n",
    "            if policy[state] != best_action:\n",
    "                stable = False\n",
    "                policy[state] = best_action\n",
    "                \n",
    "        # Stop if the policy is stable (converged)\n",
    "        if stable:\n",
    "            return policy, V\n",
    "\n",
    "def visualize_grid(mdp: GridWorldMDP, path: List[Tuple[int, int]] = None):\n",
    "    \"\"\"Visualize the grid world with the path taken.\"\"\"\n",
    "    grid = [[' ' for _ in range(mdp.size)] for _ in range(mdp.size)]\n",
    "    \n",
    "    # Place obstacles\n",
    "    for obs in mdp.obstacles:\n",
    "        grid[obs[0]][obs[1]] = 'â–ˆ'\n",
    "    \n",
    "    # Place start and goal\n",
    "    grid[mdp.start_state[0]][mdp.start_state[1]] = 'S'\n",
    "    grid[mdp.goal_state[0]][mdp.goal_state[1]] = 'G'\n",
    "    \n",
    "    # Place path\n",
    "    if path:\n",
    "        for state in path[1:-1]:  # Exclude start and goal states\n",
    "            if state != mdp.goal_state and state != mdp.start_state:\n",
    "                grid[state[0]][state[1]] = 'â€¢'\n",
    "    \n",
    "    # Print the grid\n",
    "    print('\\n'.join([' '.join(row) for row in grid]))\n",
    "\n",
    "def simulate_episode(mdp: GridWorldMDP, policy: Dict[Tuple[int, int], str], max_steps: int = 100) -> Tuple[List[Tuple[int, int]], bool]:\n",
    "    \"\"\"Simulate one episode following the given policy with some randomness.\"\"\"\n",
    "    current_state = mdp.start_state\n",
    "    path = [current_state]\n",
    "    steps = 0\n",
    "    \n",
    "    while steps < max_steps:\n",
    "        if current_state == mdp.goal_state:\n",
    "            return path, True\n",
    "            \n",
    "        # Add some randomness to make it more dynamic\n",
    "        if random.random() < 0.2:  # 20% chance of random action\n",
    "            action = random.choice(mdp.actions)\n",
    "        else:\n",
    "            action = policy[current_state]\n",
    "            \n",
    "        current_state = mdp.get_next_state(current_state, action)\n",
    "        path.append(current_state)\n",
    "        steps += 1\n",
    "    \n",
    "    return path, False\n",
    "\n",
    "# Now you can call policy_iteration like this:\n",
    "# mdp = GridWorldMDP(size=5)\n",
    "# optimal_policy, optimal_values = policy_iteration(mdp)\n",
    "\n",
    "def run_simulation():\n",
    "    \"\"\"Run a complete simulation using policy iteration.\"\"\"\n",
    "    # Create MDP\n",
    "    mdp = GridWorldMDP(size=4)\n",
    "    \n",
    "    print(\"Initial Grid:\")\n",
    "    visualize_grid(mdp)\n",
    "    \n",
    "    # Compute optimal policy using policy iteration\n",
    "    print(\"\\nComputing optimal policy using Policy Iteration...\")\n",
    "    optimal_policy, optimal_values = policy_iteration(mdp)\n",
    "    \n",
    "    # Simulate episode\n",
    "    print(\"\\nSimulating episode using optimal policy...\")\n",
    "    path, success = simulate_episode(mdp, optimal_policy)\n",
    "    \n",
    "    print(\"\\nFinal Path:\")\n",
    "    visualize_grid(mdp, path)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nSuccess! Agent reached the goal! ðŸŽ‰\")\n",
    "    else:\n",
    "        print(\"\\nBetter luck next time! Agent didn't reach the goal in time. ðŸ˜ž\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nPath length: {len(path)} steps\")\n",
    "    print(f\"Start state: {mdp.start_state}\")\n",
    "    print(f\"Goal state: {mdp.goal_state}\")\n",
    "\n",
    "# You can run a simulation using the policy iteration function:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Policy Iteration Simulation ===\")\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(mdp: GridWorldMDP, V: Dict[Tuple[int, int], float], gamma: float = 0.9) -> Dict[Tuple[int, int], str]:\n",
    "    \"\"\"Perform policy improvement given the current value function.\"\"\"\n",
    "    new_policy = {}\n",
    "\n",
    "    for state in mdp.states:\n",
    "        if state == mdp.goal_state:\n",
    "            new_policy[state] = None  # No action needed for the goal state\n",
    "            continue\n",
    "\n",
    "        # Find the best action based on the current value function\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in mdp.actions:\n",
    "            next_state = mdp.get_next_state(state, action)\n",
    "            value = mdp.get_reward(next_state) + gamma * V[next_state]\n",
    "\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        new_policy[state] = best_action\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating initial policy...\n",
      "Policy evaluation complete.\n",
      "Initial state-value function:\n",
      "State: (0, 0), Value: -10.00\n",
      "State: (0, 1), Value: -10.00\n",
      "State: (0, 2), Value: -10.00\n",
      "State: (0, 3), Value: -10.00\n",
      "State: (0, 4), Value: -10.00\n",
      "State: (1, 0), Value: -10.00\n",
      "State: (1, 1), Value: -10.00\n",
      "State: (1, 2), Value: -10.00\n",
      "State: (1, 3), Value: -10.00\n",
      "State: (1, 4), Value: -10.00\n",
      "State: (2, 0), Value: -10.00\n",
      "State: (2, 1), Value: -10.00\n",
      "State: (2, 2), Value: -10.00\n",
      "State: (2, 3), Value: -10.00\n",
      "State: (2, 4), Value: -10.00\n",
      "State: (3, 0), Value: -10.00\n",
      "State: (3, 1), Value: -10.00\n",
      "State: (3, 2), Value: -10.00\n",
      "State: (3, 3), Value: 89.00\n",
      "State: (3, 4), Value: 100.00\n",
      "State: (4, 0), Value: -10.00\n",
      "State: (4, 1), Value: -10.00\n",
      "State: (4, 2), Value: 89.00\n",
      "State: (4, 3), Value: 100.00\n",
      "State: (4, 4), Value: 0.00\n",
      "\n",
      "Improving policy...\n",
      "Policy improvement complete.\n",
      "\n",
      "Initial policy:\n",
      "State: (0, 0), Action: up\n",
      "State: (0, 1), Action: up\n",
      "State: (0, 2), Action: up\n",
      "State: (0, 3), Action: up\n",
      "State: (0, 4), Action: down\n",
      "State: (1, 0), Action: up\n",
      "State: (1, 1), Action: up\n",
      "State: (1, 2), Action: up\n",
      "State: (1, 3), Action: up\n",
      "State: (1, 4), Action: up\n",
      "State: (2, 0), Action: up\n",
      "State: (2, 1), Action: up\n",
      "State: (2, 2), Action: up\n",
      "State: (2, 3), Action: up\n",
      "State: (2, 4), Action: up\n",
      "State: (3, 0), Action: up\n",
      "State: (3, 1), Action: up\n",
      "State: (3, 2), Action: up\n",
      "State: (3, 3), Action: down\n",
      "State: (3, 4), Action: down\n",
      "State: (4, 0), Action: up\n",
      "State: (4, 1), Action: up\n",
      "State: (4, 2), Action: right\n",
      "State: (4, 3), Action: right\n",
      "State: (4, 4), Action: None\n",
      "\n",
      "Improved policy:\n",
      "State: (0, 0), Action: up\n",
      "State: (0, 1), Action: up\n",
      "State: (0, 2), Action: up\n",
      "State: (0, 3), Action: up\n",
      "State: (0, 4), Action: down\n",
      "State: (1, 0), Action: up\n",
      "State: (1, 1), Action: up\n",
      "State: (1, 2), Action: up\n",
      "State: (1, 3), Action: up\n",
      "State: (1, 4), Action: up\n",
      "State: (2, 0), Action: up\n",
      "State: (2, 1), Action: up\n",
      "State: (2, 2), Action: up\n",
      "State: (2, 3), Action: up\n",
      "State: (2, 4), Action: up\n",
      "State: (3, 0), Action: up\n",
      "State: (3, 1), Action: up\n",
      "State: (3, 2), Action: up\n",
      "State: (3, 3), Action: down\n",
      "State: (3, 4), Action: down\n",
      "State: (4, 0), Action: up\n",
      "State: (4, 1), Action: up\n",
      "State: (4, 2), Action: right\n",
      "State: (4, 3), Action: right\n",
      "State: (4, 4), Action: None\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of GridWorldMDP\n",
    "mdp = GridWorldMDP(size=5)\n",
    "\n",
    "# Run value iteration to get the value function\n",
    "values = value_iteration(mdp)\n",
    "\n",
    "# Get the initial policy based on the value function\n",
    "initial_policy = get_optimal_policy(mdp, values)\n",
    "\n",
    "# Display initial state-value function\n",
    "print(\"Evaluating initial policy...\")\n",
    "print(\"Policy evaluation complete.\")\n",
    "print(\"Initial state-value function:\")\n",
    "for state, value in values.items():\n",
    "    print(f\"State: {state}, Value: {value:.2f}\")\n",
    "\n",
    "# Apply policy improvement using the value function\n",
    "print(\"\\nImproving policy...\")\n",
    "improved_policy = policy_improvement(mdp, values)\n",
    "print(\"Policy improvement complete.\")\n",
    "\n",
    "# Display initial and improved policy\n",
    "print(\"\\nInitial policy:\")\n",
    "for state, action in initial_policy.items():\n",
    "    print(f\"State: {state}, Action: {action}\")\n",
    "\n",
    "print(\"\\nImproved policy:\")\n",
    "for state, action in improved_policy.items():\n",
    "    print(f\"State: {state}, Action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
