{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    def __init__(self, size: int = 5, obstacles: List[Tuple[int, int]] = None):\n",
    "        self.size = size\n",
    "        self.states = [(i, j) for i in range(size) for j in range(size)]\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        # Randomly place start and goal states\n",
    "        self.start_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        self.goal_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        \n",
    "        # Make sure start and goal states are different\n",
    "        while self.goal_state == self.start_state:\n",
    "            self.goal_state = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "            \n",
    "        # Set obstacles\n",
    "        self.obstacles = obstacles if obstacles else []\n",
    "        while len(self.obstacles) < size:  # Add random obstacles\n",
    "            obs = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "            if obs not in self.obstacles and obs != self.start_state and obs != self.goal_state:\n",
    "                self.obstacles.append(obs)\n",
    "\n",
    "    def get_next_state(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "        \"\"\"Determine next state given current state and action.\"\"\"\n",
    "        x, y = state\n",
    "        if action == 'up':\n",
    "            new_state = (max(0, x-1), y)\n",
    "        elif action == 'down':\n",
    "            new_state = (min(self.size-1, x+1), y)\n",
    "        elif action == 'left':\n",
    "            new_state = (x, max(0, y-1))\n",
    "        else:  # right\n",
    "            new_state = (x, min(self.size-1, y+1))\n",
    "            \n",
    "        # Check if new state is an obstacle\n",
    "        if new_state in self.obstacles:\n",
    "            return state\n",
    "        return new_state\n",
    "\n",
    "    def get_reward(self, state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Get reward for being in a state.\"\"\"\n",
    "        if state == self.goal_state:\n",
    "            return 100\n",
    "        elif state in self.obstacles:\n",
    "            return -50\n",
    "        return -1  # Small negative reward for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: GridWorldMDP, gamma: float = 0.9, epsilon: float = 1e-6) -> Dict[Tuple[int, int], float]:\n",
    "    \"\"\"Perform value iteration to find optimal values for each state.\"\"\"\n",
    "    V = {state: 0 for state in mdp.states}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if state == mdp.goal_state:\n",
    "                continue\n",
    "                \n",
    "            # Find maximum value over all actions\n",
    "            max_value = float('-inf')\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                value = mdp.get_reward(next_state) + gamma * V[next_state]\n",
    "                max_value = max(max_value, value)\n",
    "            \n",
    "            V_new[state] = max_value\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_policy(mdp: GridWorldMDP, V: Dict[Tuple[int, int], float], gamma: float = 0.9) -> Dict[Tuple[int, int], str]:\n",
    "    \"\"\"Extract optimal policy from value function.\"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in mdp.states:\n",
    "        if state == mdp.goal_state:\n",
    "            policy[state] = None\n",
    "            continue\n",
    "            \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in mdp.actions:\n",
    "            next_state = mdp.get_next_state(state, action)\n",
    "            value = mdp.get_reward(next_state) + gamma * V[next_state]\n",
    "            \n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "                \n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(mdp: GridWorldMDP, policy: Dict[Tuple[int, int], str], max_steps: int = 100) -> Tuple[List[Tuple[int, int]], bool]:\n",
    "    \"\"\"Simulate one episode following the given policy with some randomness.\"\"\"\n",
    "    current_state = mdp.start_state\n",
    "    path = [current_state]\n",
    "    steps = 0\n",
    "    \n",
    "    while steps < max_steps:\n",
    "        if current_state == mdp.goal_state:\n",
    "            return path, True\n",
    "            \n",
    "        # Add some randomness to make it more dynamic\n",
    "        if random.random() < 0.2:  # 20% chance of random action\n",
    "            action = random.choice(mdp.actions)\n",
    "        else:\n",
    "            action = policy[current_state]\n",
    "            \n",
    "        current_state = mdp.get_next_state(current_state, action)\n",
    "        path.append(current_state)\n",
    "        steps += 1\n",
    "    \n",
    "    return path, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid(mdp: GridWorldMDP, path: List[Tuple[int, int]] = None):\n",
    "    \"\"\"Visualize the grid world with the path taken.\"\"\"\n",
    "    grid = [[' ' for _ in range(mdp.size)] for _ in range(mdp.size)]\n",
    "    \n",
    "    # Place obstacles\n",
    "    for obs in mdp.obstacles:\n",
    "        grid[obs[0]][obs[1]] = 'â–ˆ'\n",
    "    \n",
    "    # Place start and goal\n",
    "    grid[mdp.start_state[0]][mdp.start_state[1]] = 'S'\n",
    "    grid[mdp.goal_state[0]][mdp.goal_state[1]] = 'G'\n",
    "    \n",
    "    # Place path\n",
    "    if path:\n",
    "        for state in path[1:-1]:  # Exclude start and goal states\n",
    "            if state != mdp.goal_state and state != mdp.start_state:\n",
    "                grid[state[0]][state[1]] = 'â€¢'\n",
    "    \n",
    "    # Print the grid\n",
    "    print('\\n'.join([' '.join(row) for row in grid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"Run a complete simulation.\"\"\"\n",
    "    # Create MDP\n",
    "    mdp = GridWorldMDP(size=4)\n",
    "    \n",
    "    print(\"Initial Grid:\")\n",
    "    visualize_grid(mdp)\n",
    "    \n",
    "    # Compute optimal values and policy\n",
    "    print(\"\\nComputing optimal policy...\")\n",
    "    values = value_iteration(mdp)\n",
    "    policy = get_optimal_policy(mdp, values)\n",
    "    \n",
    "    # Simulate episode\n",
    "    print(\"\\nSimulating episode...\")\n",
    "    path, success = simulate_episode(mdp, policy)\n",
    "    \n",
    "    print(\"\\nFinal Path:\")\n",
    "    visualize_grid(mdp, path)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nSuccess! Agent reached the goal! ðŸŽ‰\")\n",
    "    else:\n",
    "        print(\"\\nBetter luck next time! Agent didn't reach the goal in time. ðŸ˜ž\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nPath length: {len(path)} steps\")\n",
    "    print(f\"Start state: {mdp.start_state}\")\n",
    "    print(f\"Goal state: {mdp.goal_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple simulations\n",
    "def run_multiple_simulations(num_simulations: int = 5):\n",
    "    \"\"\"Run multiple simulations and track success rate.\"\"\"\n",
    "    successes = 0\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        print(f\"\\n=== Simulation {i+1} ===\")\n",
    "        mdp = GridWorldMDP(size=5)\n",
    "        values = value_iteration(mdp)\n",
    "        policy = get_optimal_policy(mdp, values)\n",
    "        path, success = simulate_episode(mdp, policy)\n",
    "        \n",
    "        visualize_grid(mdp, path)\n",
    "        if success:\n",
    "            successes += 1\n",
    "            print(\"\\nSuccess! Agent reached the goal! ðŸŽ‰\")\n",
    "        else:\n",
    "            print(\"\\nBetter luck next time! Agent didn't reach the goal in time. ðŸ˜ž\")\n",
    "        \n",
    "        time.sleep(1)  # Add delay between simulations\n",
    "    \n",
    "    print(f\"\\nOverall Success Rate: {successes/num_simulations*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single Simulation ===\n",
      "Initial Grid:\n",
      "    G  \n",
      "       \n",
      "â–ˆ â–ˆ S  \n",
      "â–ˆ     â–ˆ\n",
      "\n",
      "Computing optimal policy...\n",
      "\n",
      "Simulating episode...\n",
      "\n",
      "Final Path:\n",
      "    G  \n",
      "    â€¢  \n",
      "â–ˆ â–ˆ S  \n",
      "â–ˆ     â–ˆ\n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "Path length: 3 steps\n",
      "Start state: (2, 2)\n",
      "Goal state: (0, 2)\n",
      "\n",
      "=== Multiple Simulations ===\n",
      "\n",
      "=== Simulation 1 ===\n",
      "    â–ˆ â–ˆ â–ˆ\n",
      "G        \n",
      "S â–ˆ     â–ˆ\n",
      "         \n",
      "         \n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "=== Simulation 2 ===\n",
      "  S      \n",
      "â–ˆ â€¢     â–ˆ\n",
      "  â€¢   â–ˆ  \n",
      "â€¢ â€¢ G    \n",
      "â–ˆ     â–ˆ  \n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "=== Simulation 3 ===\n",
      "    â–ˆ   â–ˆ\n",
      "  S     â–ˆ\n",
      "  â€¢      \n",
      "  â€¢   â–ˆ  \n",
      "  â€¢ G   â–ˆ\n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "=== Simulation 4 ===\n",
      "  S G    \n",
      "         \n",
      "    â–ˆ   â–ˆ\n",
      "    â–ˆ   â–ˆ\n",
      "        â–ˆ\n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "=== Simulation 5 ===\n",
      "    â–ˆ    \n",
      "â–ˆ â€¢ â€¢ S  \n",
      "G â€¢ â–ˆ â–ˆ  \n",
      "  â–ˆ      \n",
      "         \n",
      "\n",
      "Success! Agent reached the goal! ðŸŽ‰\n",
      "\n",
      "Overall Success Rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run a single simulation\n",
    "    print(\"=== Single Simulation ===\")\n",
    "    run_simulation()\n",
    "    \n",
    "    # Run multiple simulations\n",
    "    print(\"\\n=== Multiple Simulations ===\")\n",
    "    run_multiple_simulations(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
